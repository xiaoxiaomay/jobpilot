{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Test Analysis Framework\n",
    "\n",
    "**Author**: Xiaoxiao Wu  \n",
    "**Purpose**: Reusable A/B testing analysis module with:\n",
    "- Power analysis & sample size calculation\n",
    "- Two-sample t-test & chi-square test\n",
    "- Sequential testing (always-valid p-values)\n",
    "- Uplift modeling for heterogeneous treatment effects\n",
    "- Visualization of results\n",
    "\n",
    "**Context**: Built from experience running large-scale experiments at Uber China (pricing elasticity, dispatching algorithms, user segmentation campaigns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.stats.power import TTestIndPower, NormalIndPower\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Power Analysis & Sample Size Calculation\n",
    "\n",
    "Before running any experiment, we need to determine the minimum sample size to detect a meaningful effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(\n",
    "    baseline_rate: float,\n",
    "    min_detectable_effect: float,\n",
    "    alpha: float = 0.05,\n",
    "    power: float = 0.80,\n",
    "    test_type: str = 'proportions'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calculate minimum sample size per group for an A/B test.\n",
    "    \n",
    "    Args:\n",
    "        baseline_rate: Current conversion rate (e.g., 0.05 for 5%)\n",
    "        min_detectable_effect: Minimum relative lift to detect (e.g., 0.10 for 10% lift)\n",
    "        alpha: Significance level (Type I error rate)\n",
    "        power: Statistical power (1 - Type II error rate)\n",
    "        test_type: 'proportions' or 'continuous'\n",
    "    \n",
    "    Returns:\n",
    "        dict with sample size and experiment parameters\n",
    "    \"\"\"\n",
    "    treatment_rate = baseline_rate * (1 + min_detectable_effect)\n",
    "    \n",
    "    if test_type == 'proportions':\n",
    "        # Cohen's h for proportions\n",
    "        h = 2 * np.arcsin(np.sqrt(treatment_rate)) - 2 * np.arcsin(np.sqrt(baseline_rate))\n",
    "        analysis = NormalIndPower()\n",
    "        n = analysis.solve_power(effect_size=h, alpha=alpha, power=power, alternative='two-sided')\n",
    "    else:\n",
    "        # Cohen's d for continuous metrics\n",
    "        d = min_detectable_effect  # Standardized effect size\n",
    "        analysis = TTestIndPower()\n",
    "        n = analysis.solve_power(effect_size=d, alpha=alpha, power=power, alternative='two-sided')\n",
    "    \n",
    "    n_per_group = int(np.ceil(n))\n",
    "    \n",
    "    return {\n",
    "        'sample_size_per_group': n_per_group,\n",
    "        'total_sample_size': n_per_group * 2,\n",
    "        'baseline_rate': baseline_rate,\n",
    "        'expected_treatment_rate': treatment_rate,\n",
    "        'min_detectable_effect': min_detectable_effect,\n",
    "        'alpha': alpha,\n",
    "        'power': power,\n",
    "    }\n",
    "\n",
    "# Example: Current conversion rate is 5%, want to detect 10% relative lift\n",
    "result = calculate_sample_size(baseline_rate=0.05, min_detectable_effect=0.10)\n",
    "print(f\"Sample size needed per group: {result['sample_size_per_group']:,}\")\n",
    "print(f\"Total participants needed:    {result['total_sample_size']:,}\")\n",
    "print(f\"Detecting: {result['baseline_rate']*100:.1f}% → {result['expected_treatment_rate']*100:.1f}% ({result['min_detectable_effect']*100:.0f}% lift)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A/B Test Analysis — Conversion Rate (Proportions Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ab_test_proportions(\n",
    "    control_conversions: int, control_total: int,\n",
    "    treatment_conversions: int, treatment_total: int,\n",
    "    alpha: float = 0.05\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Analyze an A/B test for binary outcomes (conversion rates).\n",
    "    Uses two-proportions z-test.\n",
    "    \"\"\"\n",
    "    # Conversion rates\n",
    "    p_control = control_conversions / control_total\n",
    "    p_treatment = treatment_conversions / treatment_total\n",
    "    lift = (p_treatment - p_control) / p_control\n",
    "    \n",
    "    # Two-proportions z-test\n",
    "    count = np.array([treatment_conversions, control_conversions])\n",
    "    nobs = np.array([treatment_total, control_total])\n",
    "    z_stat, p_value = proportions_ztest(count, nobs, alternative='two-sided')\n",
    "    \n",
    "    # Confidence interval for the difference\n",
    "    se = np.sqrt(p_control * (1 - p_control) / control_total + \n",
    "                 p_treatment * (1 - p_treatment) / treatment_total)\n",
    "    z_crit = stats.norm.ppf(1 - alpha / 2)\n",
    "    diff = p_treatment - p_control\n",
    "    ci_lower = diff - z_crit * se\n",
    "    ci_upper = diff + z_crit * se\n",
    "    \n",
    "    significant = p_value < alpha\n",
    "    \n",
    "    return {\n",
    "        'control_rate': p_control,\n",
    "        'treatment_rate': p_treatment,\n",
    "        'absolute_diff': diff,\n",
    "        'relative_lift': lift,\n",
    "        'z_statistic': z_stat,\n",
    "        'p_value': p_value,\n",
    "        'ci_95': (ci_lower, ci_upper),\n",
    "        'significant': significant,\n",
    "        'recommendation': 'SHIP IT ✅' if significant and lift > 0 else \n",
    "                         ('REVERT ❌' if significant and lift < 0 else 'INCONCLUSIVE ⚠️')\n",
    "    }\n",
    "\n",
    "# Example test result\n",
    "result = analyze_ab_test_proportions(\n",
    "    control_conversions=500, control_total=10000,\n",
    "    treatment_conversions=560, treatment_total=10000\n",
    ")\n",
    "\n",
    "print(f\"Control rate:    {result['control_rate']:.2%}\")\n",
    "print(f\"Treatment rate:  {result['treatment_rate']:.2%}\")\n",
    "print(f\"Relative lift:   {result['relative_lift']:+.1%}\")\n",
    "print(f\"P-value:         {result['p_value']:.4f}\")\n",
    "print(f\"95% CI:          [{result['ci_95'][0]:.4f}, {result['ci_95'][1]:.4f}]\")\n",
    "print(f\"Significant:     {result['significant']}\")\n",
    "print(f\"Recommendation:  {result['recommendation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A/B Test Analysis — Continuous Metrics (Revenue, Time-on-Trip, etc.)\n",
    "\n",
    "For continuous metrics like revenue per user, average order value, or time-on-trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ab_test_continuous(\n",
    "    control_data: np.ndarray,\n",
    "    treatment_data: np.ndarray,\n",
    "    alpha: float = 0.05,\n",
    "    use_bootstrap: bool = False,\n",
    "    n_bootstrap: int = 10000\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Analyze A/B test for continuous outcomes.\n",
    "    Supports parametric (Welch's t-test) and non-parametric (bootstrap) approaches.\n",
    "    \"\"\"\n",
    "    mean_c = np.mean(control_data)\n",
    "    mean_t = np.mean(treatment_data)\n",
    "    lift = (mean_t - mean_c) / mean_c if mean_c != 0 else np.inf\n",
    "    \n",
    "    if use_bootstrap:\n",
    "        # Bootstrap confidence interval\n",
    "        diffs = []\n",
    "        for _ in range(n_bootstrap):\n",
    "            boot_c = np.random.choice(control_data, size=len(control_data), replace=True)\n",
    "            boot_t = np.random.choice(treatment_data, size=len(treatment_data), replace=True)\n",
    "            diffs.append(np.mean(boot_t) - np.mean(boot_c))\n",
    "        \n",
    "        ci_lower = np.percentile(diffs, 100 * alpha / 2)\n",
    "        ci_upper = np.percentile(diffs, 100 * (1 - alpha / 2))\n",
    "        p_value = np.mean(np.array(diffs) <= 0) * 2  # Two-sided\n",
    "        p_value = min(p_value, 2 - p_value)\n",
    "        method = 'Bootstrap'\n",
    "    else:\n",
    "        # Welch's t-test (unequal variances)\n",
    "        t_stat, p_value = stats.ttest_ind(treatment_data, control_data, equal_var=False)\n",
    "        se = np.sqrt(np.var(control_data)/len(control_data) + np.var(treatment_data)/len(treatment_data))\n",
    "        z_crit = stats.t.ppf(1 - alpha/2, df=min(len(control_data), len(treatment_data)) - 1)\n",
    "        diff = mean_t - mean_c\n",
    "        ci_lower = diff - z_crit * se\n",
    "        ci_upper = diff + z_crit * se\n",
    "        method = \"Welch's t-test\"\n",
    "    \n",
    "    significant = p_value < alpha\n",
    "    \n",
    "    return {\n",
    "        'control_mean': mean_c,\n",
    "        'treatment_mean': mean_t,\n",
    "        'absolute_diff': mean_t - mean_c,\n",
    "        'relative_lift': lift,\n",
    "        'p_value': p_value,\n",
    "        'ci_95': (ci_lower, ci_upper),\n",
    "        'significant': significant,\n",
    "        'method': method,\n",
    "        'recommendation': 'SHIP IT ✅' if significant and lift > 0 else\n",
    "                         ('REVERT ❌' if significant and lift < 0 else 'INCONCLUSIVE ⚠️')\n",
    "    }\n",
    "\n",
    "# Simulate revenue data\n",
    "control_rev = np.random.lognormal(mean=3.5, sigma=0.8, size=5000)\n",
    "treatment_rev = np.random.lognormal(mean=3.55, sigma=0.8, size=5000)\n",
    "\n",
    "result = analyze_ab_test_continuous(control_rev, treatment_rev)\n",
    "print(f\"Control mean:    ${result['control_mean']:.2f}\")\n",
    "print(f\"Treatment mean:  ${result['treatment_mean']:.2f}\")\n",
    "print(f\"Relative lift:   {result['relative_lift']:+.1%}\")\n",
    "print(f\"P-value:         {result['p_value']:.4f}\")\n",
    "print(f\"Method:          {result['method']}\")\n",
    "print(f\"Recommendation:  {result['recommendation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ab_results(result: dict, metric_name: str = 'Conversion Rate'):\n",
    "    \"\"\"Visualize A/B test results with confidence intervals.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Bar chart with CI\n",
    "    ax = axes[0]\n",
    "    groups = ['Control', 'Treatment']\n",
    "    means = [result.get('control_rate', result.get('control_mean')),\n",
    "             result.get('treatment_rate', result.get('treatment_mean'))]\n",
    "    colors = ['#2196F3', '#4CAF50' if result['relative_lift'] > 0 else '#F44336']\n",
    "    \n",
    "    bars = ax.bar(groups, means, color=colors, alpha=0.8, width=0.5)\n",
    "    ax.set_ylabel(metric_name)\n",
    "    ax.set_title(f'{metric_name}: Control vs Treatment')\n",
    "    \n",
    "    for bar, val in zip(bars, means):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                f'{val:.2%}' if val < 1 else f'${val:.2f}',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Effect size with CI\n",
    "    ax = axes[1]\n",
    "    ci = result['ci_95']\n",
    "    diff = result['absolute_diff']\n",
    "    \n",
    "    ax.errorbar(0, diff, yerr=[[diff - ci[0]], [ci[1] - diff]],\n",
    "                fmt='o', markersize=10, capsize=8, capthick=2, linewidth=2,\n",
    "                color='green' if result['significant'] else 'gray')\n",
    "    ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel(f'Difference in {metric_name}')\n",
    "    ax.set_title(f\"Effect Size (p={result['p_value']:.4f}) — {result['recommendation']}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/ab_test_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# plot_ab_results(result, metric_name='Revenue per User')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multiple Testing Correction (Bonferroni & FDR)\n",
    "\n",
    "When running multiple metrics simultaneously, correct for false discoveries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def correct_multiple_tests(p_values: list, method: str = 'fdr_bh', alpha: float = 0.05):\n",
    "    \"\"\"\n",
    "    Correct for multiple hypothesis testing.\n",
    "    Methods: 'bonferroni', 'fdr_bh' (Benjamini-Hochberg), 'holm'\n",
    "    \"\"\"\n",
    "    reject, corrected_p, _, _ = multipletests(p_values, alpha=alpha, method=method)\n",
    "    return pd.DataFrame({\n",
    "        'original_p': p_values,\n",
    "        'corrected_p': corrected_p,\n",
    "        'significant': reject,\n",
    "        'method': method\n",
    "    })\n",
    "\n",
    "# Example: testing 5 metrics simultaneously\n",
    "p_values = [0.003, 0.012, 0.048, 0.15, 0.72]\n",
    "metrics = ['Conversion Rate', 'Revenue/User', 'Bounce Rate', 'Session Duration', 'Page Views']\n",
    "\n",
    "results = correct_multiple_tests(p_values)\n",
    "results.index = metrics\n",
    "print(\"Multiple Testing Correction (Benjamini-Hochberg FDR):\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
